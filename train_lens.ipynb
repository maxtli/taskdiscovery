{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from training_utils import load_model_data \n","from training_utils import LinePlot\n","from lens import get_lens_loss\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","import pickle\n","import math\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load model and data\n","We will be using gpt2 small for now."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5gosREQU1_fm"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"810849eea4f84418922ca77032fb3eae","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0e503a870e0483b98ce8ab5e477274a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"029a70347ab14efe8be17ff8eb611e3b","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49aec8f476dd476fba4b481211ce02b9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abd9b735fa01449fa4d0ff2e2f89fe3d","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71999a8410c54f9f982491351ed8bd8c","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"754fd911f3bc4ab987f28697662480eb","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae5cb034983e475cad1e70c9a60bf213","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/366 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"262bfb8fe4ce4cdaafd50b1cdc7e9592","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/303M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5501a89010ac4c88853b235a2dae68ae","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e37f2d4dca6e45b6b758a75e24cb597d","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=4):   0%|          | 0/100000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (61422 > 1024). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (58948 > 1024). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (59558 > 1024). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (71857 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["12 12 64 768\n"]}],"source":["model_name = \"gpt2-small\"\n","batch_size = 32\n","device, model, tokenizer, owt_iter = load_model_data(model_name, batch_size)\n","n_layers = model.cfg.n_layers\n","n_heads = model.cfg.n_heads\n","head_dim = model.cfg.d_head\n","d_model = model.cfg.d_model\n","lr = 1e-2\n","\n","folder = \"./results/\"\n","shared_bias = False\n","\n","print(n_layers, n_heads, head_dim, d_model)"]},{"cell_type":"markdown","metadata":{},"source":["## Create Modal Lens Prameters"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["prior_bias = [model.blocks[i].attn.b_O.clone() for i in range(n_layers)]\n","\n","attn_bias = [\n","    torch.nn.Parameter(\n","        (prior_bias[i] if shared_bias else prior_bias[i].repeat(i + 1, 1)).to(device)\n","    )\n","    for i in range(n_layers)\n","]\n","\n","lens_optimizer = torch.optim.AdamW(attn_bias, lr=lr, weight_decay=0)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/5000 [00:00<?, ?it/s]\n"]},{"ename":"TypeError","evalue":"get_lens_loss() missing 4 required positional arguments: 'model', 'n_layers', 'attn_bias', and 'batch_size'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(owt_iter)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m lens_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m kl_losses, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_lens_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m kl_losses \u001b[38;5;241m=\u001b[39m kl_losses\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(kl_losses\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: get_lens_loss() missing 4 required positional arguments: 'model', 'n_layers', 'attn_bias', and 'batch_size'"]}],"source":["lp = LinePlot([*[f\"kl_loss_{k}\" for k in range(n_layers)], \"step_size\"])\n","\n","for i in tqdm(range(5000)):\n","    batch = next(owt_iter)[\"tokens\"]\n","    lens_optimizer.zero_grad()\n","\n","    kl_losses, _ = get_lens_loss(batch, model, n_layers, attn_bias,batch_size)\n","    kl_losses = kl_losses.mean(dim=-1)\n","    loss = torch.clamp(kl_losses.sum(), max=100)\n","    loss.backward()\n","\n","    prev_weights = torch.cat(attn_bias, dim=0).detach()\n","\n","    lens_optimizer.step()\n","\n","    step_sz = (torch.cat(attn_bias, dim=0) - prev_weights).abs().sum()\n","    lp.add_entry(\n","        {\n","            \"step_size\": step_sz.item(),\n","            **{f\"kl_loss_{k}\": kl_losses[k].item() for k in range(n_layers)},\n","        }\n","    )\n","\n","    # lens_scheduler.step()\n","\n","    if math.isnan(lp.stat_book[\"step_size\"][-1]):\n","        break\n","\n","    if i % 250 == 0:\n","        lp.plot(subplots=3, save=f\"{folder}/train.png\", twinx=False, mv=20)\n","        with open(f\"{folder}/modal_lens_weights.pkl\", \"wb\") as f:\n","            pickle.dump(attn_bias, f)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"td","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
